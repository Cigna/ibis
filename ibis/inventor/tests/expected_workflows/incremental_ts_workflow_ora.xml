<workflow-app name="test_workflow" xmlns="uri:oozie:workflow:0.4">
    <global>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>mapred.job.queue.name</name>
                <value>${queueName}</value>
            </property>
        </configuration>
    </global>
     <credentials>
        <credential name="hive2" type="hive2">
            <property>
                <name>hive2.jdbc.url</name>
                <value>${hive2_jdbc_url}</value>
            </property>
            <property>
                <name>hive2.server.principal</name>
                <value>${hive2_principal}</value>
            </property>
            <property>
                <name>jdbc-url</name>
                <value>${hive2_jdbc_url}</value>
            </property>
        </credential>
    </credentials>
    <start to="import_prep_fake_database_heavy_3"/>
    <action name="import_prep_fake_database_heavy_3">
        <shell xmlns="uri:oozie:shell-action:0.3">
           <exec>incr_import_prep.sh</exec>
           <env-var>source_database_name=fake_database</env-var>
           <env-var>source_table_name=heavy_3</env-var>
           <env-var>db_env=dev</env-var>
           <env-var>target_dir=mdm/test/fake_database/heavy_3</env-var>
           <env-var>hive2_jdbc_url=${hive2_jdbc_url}</env-var>
           <env-var>queueName=${queueName}</env-var>
           <env-var>incremental_mode=lastmodified</env-var>
           <env-var>it_table=ibis.dev_it_table</env-var>
           <env-var>it_table_host=fake.workflow.host</env-var>
           <env-var>hdfs_ingest_path=/user/dev/oozie/workspaces/ibis/lib/ingest/</env-var>
           <file>/user/dev/oozie/workspaces/ibis/lib/ingest/incr_import_prep.sh</file>
           <capture-output/>
        </shell>
        <ok to="incr_import_fake_database_heavy_3"/>
        <error to="oozie_cb_fail"/>
    </action>

    <action name="incr_import_fake_database_heavy_3">
        <sqoop xmlns="uri:oozie:sqoop-action:0.4">
            <configuration>
                <property>
                    <name>fs.hdfs.impl.disable.cache</name>
                    <value>true</value>
                </property>
            </configuration>
            <arg>import</arg>
            <arg>-D oraoop.timestamp.string=false</arg>
            <arg>-D hadoop.security.credential.provider.path=jceks://hdfs/user/dev/fake.passwords.jceks</arg>
            <arg>--as-avrodatafile</arg>
            <arg>--verbose</arg>
            <arg>--connect</arg>
            <arg>jdbc:oracle:thin:@//fake.oracle:1521/fake_servicename</arg>
            <arg>--target-dir</arg>
            <arg>/user/data/ingest/mdm/test/fake_database/heavy_3</arg>
            <arg>--delete-target-dir</arg>
            <arg>--table</arg>
            <arg>FAKE_DATABASE.HEAVY_3</arg>
            <arg>--username</arg>
            <arg>fake_username</arg>
            <arg>--password-alias</arg>
            <arg>fake.password.alias</arg>
            <arg>-m</arg>
            <arg>10</arg>
            <arg>--where</arg>
            <arg>TEST_INCR_COLUMN > to_timestamp('${wf:actionData('import_prep_fake_database_heavy_3')['sqoopLstUpd']}', 'YYYY-MM-DD HH24:MI:SS.FF9')</arg>
            <arg>--map-column-java</arg>
            <arg>TEST_INCR_COLUMN=String</arg>
            <arg>--fetch-size</arg>
            <arg>50000</arg>
            <arg>--direct</arg>
        </sqoop>
        <ok to="check_if_empty_fake_database_heavy_3"/>
        <error to="oozie_cb_fail"/>
    </action>
    <decision name="check_if_empty_fake_database_heavy_3">
        <switch>
            <case to="incr_post_import_fake_database_heavy_3">
                ${fs:exists('/user/data/ingest/mdm/test/fake_database/heavy_3/part-m-00000.avro')}
            </case>
            <default to="oozie_cb_ok"/>
        </switch>
    </decision>
    <action name="incr_post_import_fake_database_heavy_3">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>incr_post_import.sh</exec>
            <env-var>target_dir=mdm/test/fake_database/heavy_3</env-var>
            <env-var>hive2_jdbc_url=${hive2_jdbc_url}</env-var>
            <env-var>hdfs_ingest_path=/user/dev/oozie/workspaces/ibis/lib/ingest/</env-var>
           <file>/user/dev/oozie/workspaces/ibis/lib/ingest/incr_post_import.sh</file>
        </shell>
        <ok to="incr_create_table_fake_database_heavy_3"/>
        <error to="oozie_cb_fail"/>
    </action>
    <action cred="hive2" name="incr_create_table_fake_database_heavy_3">
        <hive2 xmlns="uri:oozie:hive2-action:0.1">
            <jdbc-url>${hive2_jdbc_url}</jdbc-url>
            <script>/user/data/mdm/test/fake_database/heavy_3/gen/incr_create_table.hql</script>
        </hive2>
        <ok to="incr_quality_assurance_fake_database_heavy_3"/>
        <error to="oozie_cb_fail"/>
    </action>

    <action name="incr_quality_assurance_fake_database_heavy_3">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>quality_assurance.sh</exec>
            <env-var>ingestion_type=incremental</env-var>
            <env-var>target_dir=mdm/test/fake_database/heavy_3</env-var>
            <env-var>HADOOP_CONF_DIR=/etc/hadoop/conf</env-var>
            <env-var>hdfs_ingest_path=/user/dev/oozie/workspaces/ibis/lib/ingest/</env-var>
            <env-var>workflowName=test_workflow</env-var>
            <file>/user/dev/oozie/workspaces/ibis/lib/ingest/quality_assurance.sh#quality_assurance.sh</file>
        </shell>
        <ok to="incr_merge_partition_fake_database_heavy_3"/>
        <error to="oozie_cb_fail"/>
    </action>
    <action cred="hive2" name="incr_merge_partition_fake_database_heavy_3">
        <hive2 xmlns="uri:oozie:hive2-action:0.1">
            <jdbc-url>${hive2_jdbc_url}</jdbc-url>
            <script>/user/data/mdm/test/fake_database/heavy_3/gen/incr_merge_partition.hql</script>
        </hive2>
        <ok to="fake_database_heavy_3_views"/>
        <error to="oozie_cb_fail"/>
    </action>
    <action cred="hive2" name="fake_database_heavy_3_views">
        <hive2 xmlns="uri:oozie:hive2-action:0.1">
        <jdbc-url>${hive2_jdbc_url}</jdbc-url>
        <script>${nameNode}/user/data/mdm/test/fake_database/heavy_3/gen/views.hql</script>
        </hive2>
        <ok to="fake_database_heavy_3_refresh"/>
        <error to="oozie_cb_fail"/>
    </action>
    <action name="fake_database_heavy_3_refresh">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>impala_cleanup.sh</exec>
            <env-var>target_dir=mdm/test/fake_database/heavy_3</env-var>
            <env-var>HADOOP_CONF_DIR=/etc/hadoop/conf</env-var>
            <env-var>hdfs_ingest_path=/user/dev/oozie/workspaces/ibis/lib/ingest/</env-var>
            <file>/user/dev/oozie/workspaces/ibis/lib/ingest/impala_cleanup.sh#impala_cleanup.sh</file>
        </shell>
        <ok to="oozie_cb_ok"/>
        <error to="oozie_cb_fail"/>
    </action>
    <action name="oozie_cb_ok">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>oozie_cb.sh</exec>
            <env-var>workflowName=test_workflow</env-var>
            <env-var>HADOOP_CONF_DIR=/etc/hadoop/conf</env-var>
            <env-var>hdfs_ingest_path=/user/dev/oozie/workspaces/ibis/lib/ingest/</env-var>
            <file>/user/dev/oozie/workspaces/ibis/lib/ingest/oozie_cb.sh#oozie_cb.sh</file>
        </shell>
        <ok to="end"/>
        <error to="kill"/>
    </action>
    <action name="oozie_cb_fail">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>oozie_cb.sh</exec>
            <env-var>workflowName=test_workflow</env-var>
            <env-var>HADOOP_CONF_DIR=/etc/hadoop/conf</env-var>
            <env-var>hdfs_ingest_path=/user/dev/oozie/workspaces/ibis/lib/ingest/</env-var>
            <file>/user/dev/oozie/workspaces/ibis/lib/ingest/oozie_cb.sh#oozie_cb.sh</file>
        </shell>
        <ok to="kill"/>
        <error to="kill"/>
    </action>

    <kill name="kill">
        <message>Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name="end"/>
</workflow-app>