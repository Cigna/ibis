<workflow-app name="test_workflow" xmlns="uri:oozie:workflow:0.4">
    <global>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>mapred.job.queue.name</name>
                <value>${queueName}</value>
            </property>
        </configuration>
    </global>
    <credentials>
        <credential name="hive2" type="hive2">
            <property>
                <name>hive2.jdbc.url</name>
                <value>${hive2_jdbc_url}</value>
            </property>
            <property>
                <name>hive2.server.principal</name>
                <value>${hive2_principal}</value>
            </property>
            <property>
                <name>jdbc-url</name>
                <value>${hive2_jdbc_url}</value>
            </property>
        </credential>
    </credentials>
    <start to="fake_database_fake_rule_tablename_genhql"/>
    <action name="fake_database_fake_rule_tablename_genhql">
        <shell xmlns="uri:oozie:shell-action:0.3">
            <exec>parquet_export.sh</exec>
            <env-var>table_name=fake_database_fake_rule_tablename</env-var>
            <env-var>target_dir=/user/data/mdm/member/fake_database/fake_rule_tablename</env-var>
            <env-var>database=member</env-var>
            <env-var>hive2_jdbc_url=${hive2_jdbc_url}</env-var>
            <env-var>HADOOP_CONF_DIR=/etc/hadoop/conf</env-var>
            <env-var>hdfs_ingest_path=/user/dev/oozie/workspaces/ibis/lib/ingest/</env-var>
            <file>/user/dev/oozie/workspaces/ibis/lib/ingest/parquet_export.sh#parquet_export.sh</file>
        </shell>
        <ok to="fake_database_fake_rule_tablename_runhql"/>
        <error to="oozie_cb_fail"/>
    </action>
    <action cred="hive2" name="fake_database_fake_rule_tablename_runhql">
        <hive2 xmlns="uri:oozie:hive2-action:0.1">
            <jdbc-url>${hive2_jdbc_url}</jdbc-url>
            <script>${nameNode}/tmp/fake_database_fake_rule_tablename_clone_hql/parquet_export.hql</script>
        </hive2>
        <ok to="fake_database_fake_rule_tablename_sqoop_export"/>
        <error to="oozie_cb_fail"/>
    </action>
    <action name="fake_database_fake_rule_tablename_sqoop_export">
        <sqoop xmlns="uri:oozie:sqoop-action:0.4">
            <configuration>
                <property>
                    <name>fs.hdfs.impl.disable.cache</name>
                    <value>true</value>
                </property>
            </configuration>
            <arg>export</arg>
            <arg>-D org.apache.sqoop.export.text.dump_data_on_error=true</arg>
            <arg>-D hadoop.security.credential.provider.path=jceks://hdfs/user/dev/fake.passwords.jceks</arg>
            <arg>--verbose</arg>
            <arg>--connect</arg>
            <arg>jdbc:oracle:thin:@//fake.oracle:1600/fake_servicename</arg>
            <arg>--username</arg>
            <arg>fake_username</arg>
            <arg>--password-alias</arg>
            <arg>fake.password.alias</arg>
            <arg>--table</arg>
            <arg>FAKE_SCHEMA.FAKE_RULE_TABLENAME</arg>
            <arg>--export-dir</arg>
            <arg>/tmp/fake_database_fake_rule_tablename_clone</arg>
            <arg>--update-key</arg>
            <arg>RULE_KEY</arg>
            <arg>--update-mode</arg>
            <arg>allowinsert</arg>
            <arg>-m</arg>
            <arg>10</arg>
            <arg>--input-null-string</arg>
            <arg>\\N</arg>
            <arg>--input-null-non-string</arg>
            <arg>\\N</arg>
            <arg>--input-fields-terminated-by</arg>
            <arg>|</arg>
        </sqoop>
        <ok to="oozie_cb_ok"/>
        <error to="kill"/>
    </action>
    <action name="oozie_cb_ok">
        <shell xmlns="uri:oozie:shell-action:0.3">
        <exec>oozie_cb.sh</exec>
        <env-var>workflowName=test_workflow</env-var>
        <env-var>HADOOP_CONF_DIR=/etc/hadoop/conf</env-var>
        <env-var>hdfs_ingest_path=/user/dev/oozie/workspaces/ibis/lib/ingest/</env-var>
        <file>/user/dev/oozie/workspaces/ibis/lib/ingest/oozie_cb.sh#oozie_cb.sh</file>
        </shell>
        <ok to="end"/>
        <error to="kill"/>
    </action>
    <action name="oozie_cb_fail">
        <shell xmlns="uri:oozie:shell-action:0.3">
        <exec>oozie_cb.sh</exec>
        <env-var>workflowName=test_workflow</env-var>
        <env-var>HADOOP_CONF_DIR=/etc/hadoop/conf</env-var>
        <env-var>hdfs_ingest_path=/user/dev/oozie/workspaces/ibis/lib/ingest/</env-var>
        <file>/user/dev/oozie/workspaces/ibis/lib/ingest/oozie_cb.sh#oozie_cb.sh</file>
        </shell>
        <ok to="kill"/>
        <error to="kill"/>
    </action>
    <kill name="kill">
        <message>Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name="end"/>
</workflow-app>